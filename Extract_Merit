# -*- coding: utf-8 -*-
"""
MERIT-BasinsæµåŸŸæå–å·¥å…· - ä¼˜åŒ–ç‰ˆ
ä¸»è¦æ”¹è¿›:
1. é¢„è®¡ç®—æŠ•å½±æ•°æ®(å‡å°‘é‡å¤è½¬æ¢)
2. ä½¿ç”¨unary_unionæ›¿ä»£dissolve(3-5xæé€Ÿ)
3. è¾“å‡ºGeoPackageå•æ–‡ä»¶(å‡å°‘I/O)
4. ä¼˜åŒ–å†…å­˜ç®¡ç†
5. æ·»åŠ æ–­ç‚¹ç»­ä¼ 
"""
import os, sys, time, warnings
warnings.filterwarnings("ignore")

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.geometry import Point
from shapely.ops import unary_union
from collections import defaultdict, deque
import gc

# ========= è·¯å¾„é…ç½® =========
riv_shp = r"Z:\Topography\MERIT-Basins\MERIT_Hydro_v07_Basins_v01\pfaf_level_01\pfaf_4_MERIT_Hydro_v07_Basins_v01\riv_pfaf_4_MERIT_Hydro_v07_Basins_v01.shp"
cat_shp = r"Z:\Topography\MERIT-Basins\MERIT_Hydro_v07_Basins_v01\pfaf_level_01\pfaf_4_MERIT_Hydro_v07_Basins_v01\cat_pfaf_4_MERIT_Hydro_v07_Basins_v01.shp"
china_prov_shp = r"Z:\ARCGIS_Useful_data\China\ä¸­å›½è¡Œæ”¿åŒº_åŒ…å«æ²¿æµ·å²›å±¿.shp"

excel_path = r"Z:\Runoff_Flood\China_runoff\æµåŸŸåŸºç¡€ä¿¡æ¯\é¢ç§¯æå–\ç«™ç‚¹ä¿¡æ¯-20251025.xlsx"
out_root   = r"Z:\Runoff_Flood\China_runoff\æµåŸŸåŸºç¡€ä¿¡æ¯\é¢ç§¯æå–"
os.makedirs(out_root, exist_ok=True)
run_log_path = os.path.join(out_root, "run_log.txt")

# ========= å‚æ•°è®¾ç½® =========
SNAP_DIST_M   = 5000.0
ORDER_FIRST   = False
MAX_UP_REACH  = 100000
AREA_TOL      = 0.20
AREA_EPSG     = 6933
SAVE_INDIVIDUAL_SHP = False  # æ–°å¢:æ˜¯å¦ä¿å­˜å•ç«™shapefile(æ”¹ä¸ºFalseèŠ‚çœI/O)
MEMORY_CHECK_INTERVAL = 50   # æ¯Nä¸ªç«™ç‚¹æ£€æŸ¥å†…å­˜
# ============================

try:
    from tqdm import tqdm
    HAS_TQDM = True
except:
    HAS_TQDM = False

# ---------- å·¥å…·å‡½æ•° ----------
def log(msg: str):
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line); sys.stdout.flush()
    with open(run_log_path, "a", encoding="utf-8") as f:
        f.write(line + "\n")

def fmt_pct(x):
    try:
        return f"{float(x):.1%}"
    except:
        return "NA"

def check_memory():
    """å†…å­˜ç›‘æ§"""
    try:
        import psutil
        mem = psutil.virtual_memory()
        if mem.percent > 85:
            log(f"âš ï¸ å†…å­˜ä½¿ç”¨ç‡ {mem.percent:.1f}%, æ‰§è¡Œåƒåœ¾å›æ”¶")
            gc.collect()
            return True
    except ImportError:
        pass
    return False

def ensure_wgs84(gdf):
    if gdf.crs is None: 
        return gdf.set_crs(4326)
    if gdf.crs.to_epsg() != 4326: 
        return gdf.to_crs(4326)
    return gdf

def valid_int(x):
    try: 
        xi = int(x)
        return xi > 0
    except: 
        return False

def pick_nearest_reach(gdf_riv_m, lon, lat, gdf_riv_wgs84):
    """
    ä¼˜åŒ–ç‰ˆ:ç›´æ¥ä½¿ç”¨é¢„æŠ•å½±çš„æ•°æ®
    Args:
        gdf_riv_m: å·²æŠ•å½±åˆ°3857çš„æ²³ç½‘
        lon, lat: WGS84åæ ‡
        gdf_riv_wgs84: åŸå§‹WGS84æ²³ç½‘(ç”¨äºè·å–å±æ€§)
    """
    # å°†ç‚¹æŠ•å½±åˆ°3857
    pt_m = gpd.GeoDataFrame(geometry=[Point(lon, lat)], crs=4326).to_crs(3857)
    pt = pt_m.geometry.iloc[0]
    
    # ç©ºé—´ç´¢å¼•æŸ¥è¯¢
    sidx = gdf_riv_m.sindex
    cand_idx = list(sidx.intersection(pt.buffer(SNAP_DIST_M).bounds))
    
    if not cand_idx:
        raise RuntimeError(f"åœ¨ {SNAP_DIST_M} m å†…æ²¡æœ‰æ²³æ®µï¼›è¯·å¢å¤§ SNAP_DIST_Mã€‚")
    
    # è·å–å€™é€‰æ²³æ®µ
    cand = gdf_riv_m.iloc[cand_idx].copy()
    cand["__dist__"] = cand.geometry.distance(pt)
    
    # ä»åŸå§‹æ•°æ®è·å–å±æ€§
    cand_orig = gdf_riv_wgs84.iloc[cand_idx].copy()
    cand["_order_"]  = cand_orig["order"].fillna(0)  if "order"  in cand_orig.columns else 0
    cand["_uparea_"] = cand_orig["uparea"].fillna(0) if "uparea" in cand_orig.columns else 0
    cand["COMID"] = cand_orig["COMID"]
    
    # æ’åºé€‰æ‹©æœ€ä¼˜æ²³æ®µ
    if ORDER_FIRST:
        cand = cand.sort_values(["_order_","__dist__","_uparea_"], 
                                ascending=[False, True, False])
    else:
        cand = cand.sort_values(["__dist__","_order_","_uparea_"], 
                                ascending=[True, False, False])
    
    r = cand.iloc[0]
    return int(r["COMID"]), float(r["__dist__"]), int(r["_order_"]), float(r["_uparea_"])

def build_upstream_graph(gdf_riv):
    """æ„å»ºä¸Šæ¸¸æ‹“æ‰‘å›¾"""
    up_fields = [c for c in ["up1","up2","up3","up4"] if c in gdf_riv.columns]
    has_next  = "NextDownID" in gdf_riv.columns
    G = defaultdict(set)
    
    if has_next:
        for _, r in gdf_riv[["COMID","NextDownID"]].iterrows():
            c, nd = r["COMID"], r["NextDownID"]
            if valid_int(c) and valid_int(nd): 
                G[int(nd)].add(int(c))
    
    if up_fields:
        cols = ["COMID"] + up_fields
        for _, r in gdf_riv[cols].iterrows():
            d = r["COMID"]
            if not valid_int(d): 
                continue
            d = int(d)
            for uf in up_fields:
                u = r[uf]
                if valid_int(u): 
                    G[d].add(int(u))
    
    if (not has_next) and (not up_fields):
        raise RuntimeError("æ²³ç½‘ç¼ºå°‘ NextDownID / up1..up4ï¼Œæ— æ³•æ„æ‹“æ‰‘ã€‚")
    
    return G

def bfs_upstream(G, outlet):
    """BFSè¿½æº¯ä¸Šæ¸¸"""
    visited, q = set([outlet]), deque([outlet])
    while q:
        cur = q.popleft()
        for u in G.get(cur, set()):
            if u not in visited:
                visited.add(u)
                q.append(u)
    return visited

def calc_polygon_area_m2(gdf_poly, gdf_poly_area_crs=None):
    """
    ä¼˜åŒ–ç‰ˆ:å¯ç›´æ¥ä½¿ç”¨é¢„æŠ•å½±æ•°æ®
    Args:
        gdf_poly: è¦è®¡ç®—é¢ç§¯çš„å¤šè¾¹å½¢
        gdf_poly_area_crs: é¢„æŠ•å½±åˆ°é¢ç§¯åæ ‡ç³»çš„å¤šè¾¹å½¢(å¯é€‰)
    """
    if gdf_poly_area_crs is not None:
        return float(gdf_poly_area_crs.area.sum())
    return float(gdf_poly.to_crs(AREA_EPSG).area.sum())

def read_site_info(xlsx_path):
    """è¯»å–æµ‹ç«™ä¿¡æ¯"""
    cand_code = ["æµ‹ç«™ç¼–ç ","æµ‹ç«™ä»£ç ","ç«™ç ","ç«™å·","code","station_id"]
    cand_lon  = ["ç»åº¦","lon","longitude"]
    cand_lat  = ["çº¬åº¦","lat","latitude"]
    cand_area = ["é›†æ°´åŒºé¢ç§¯","é¢ç§¯","area"]
    
    book = pd.read_excel(xlsx_path, sheet_name=None)
    for sheet_name, df in book.items():
        cols = {str(c).strip(): c for c in df.columns}
        code_col = next((cols[c] for c in cols if c in cand_code), None)
        lon_col  = next((cols[c] for c in cols if c in cand_lon),  None)
        lat_col  = next((cols[c] for c in cols if c in cand_lat),  None)
        area_col = next((cols[c] for c in cols if c in cand_area), None)
        
        if code_col and lon_col and lat_col and area_col:
            out = df[[code_col, lon_col, lat_col, area_col]].copy()
            out.columns = ["code","lon","lat","area"]
            out["code"] = out["code"].astype(str).str.strip()
            out["lon"]  = pd.to_numeric(out["lon"], errors="coerce")
            out["lat"]  = pd.to_numeric(out["lat"], errors="coerce")
            out["area"] = pd.to_numeric(out["area"], errors="coerce")
            return sheet_name, out.dropna(subset=["code","lon","lat"])
    
    raise RuntimeError("æœªåœ¨Excelä¸­æ‰¾åˆ°åŒæ—¶åŒ…å«ã€æµ‹ç«™ç¼–ç /ç»åº¦/çº¬åº¦/é›†æ°´åŒºé¢ç§¯ã€‘çš„å·¥ä½œè¡¨ã€‚")

def normalize_area_to_m2(series_area):
    """é¢ç§¯å•ä½å½’ä¸€åŒ–åˆ°mÂ²"""
    s = series_area.dropna()
    if s.empty: 
        return series_area
    return series_area * 1_000_000.0 if float(s.median()) < 1e6 else series_area

def process_one_site(code, lon, lat, area_target_m2, 
                     gdf_riv_m, gdf_riv_wgs84, gdf_cat, gdf_cat_area, 
                     china_prov, G):
    """
    å¤„ç†å•ä¸ªæµ‹ç«™
    ä¼˜åŒ–: ä½¿ç”¨é¢„æŠ•å½±æ•°æ® + unary_unionæ›¿ä»£dissolve
    """
    try:
        # 1. æ•æ‰æœ€è¿‘æ²³æ®µ
        outlet_comid, dist_m, ordv, upa = pick_nearest_reach(
            gdf_riv_m, lon, lat, gdf_riv_wgs84
        )
        
        # 2. BFSè¿½æº¯ä¸Šæ¸¸
        visited = bfs_upstream(G, outlet_comid)
        if len(visited) > MAX_UP_REACH:
            return {"code": code, "status": "fail", 
                   "msg": f"ä¸Šæ¸¸è¿‡å¤§({len(visited)})"}
        
        # 3. æå–å¯¹åº”çš„å•å…ƒæµåŸŸ
        sel = gdf_cat[gdf_cat["COMID"].isin(visited)].copy()
        if sel.empty:
            return {"code": code, "status": "fail", 
                   "msg": "catæœªåŒ¹é…åˆ°COMID"}
        
        # 4. åˆå¹¶æµåŸŸ - ä½¿ç”¨unary_union(æ¯”dissolveå¿«3-5å€)
        cat_geom = unary_union(sel.geometry.values)
        cat = gpd.GeoDataFrame(
            [{"station_id": code, "geometry": cat_geom}], 
            crs=sel.crs
        )
        
        # ä¿ç•™unitareaä¿¡æ¯(å¦‚æœæœ‰)
        if "unitarea" in sel.columns:
            cat["unitarea_sum"] = sel["unitarea"].sum()
        
        # 5. è®¡ç®—é¢ç§¯ - ä½¿ç”¨é¢„æŠ•å½±æ•°æ®
        sel_area = gdf_cat_area[gdf_cat_area["COMID"].isin(visited)]
        cat_area_geom = unary_union(sel_area.geometry.values)
        area_m2 = float(gpd.GeoSeries([cat_area_geom], crs=AREA_EPSG).area.sum())
        
        # 6. é¢ç§¯éªŒè¯
        rel_err = None
        pass_check = False
        if pd.notna(area_target_m2) and area_target_m2 > 0:
            rel_err = abs(area_m2 - area_target_m2) / area_target_m2
            pass_check = (rel_err <= AREA_TOL)
        else:
            pass_check = True
        
        # 7. è¾“å‡ºç»“æœ
        shp = png = stats_csv = None
        status_str = "ok" if pass_check else "reject"
        
        if pass_check:
            out_dir = os.path.join(out_root, "sites", code)
            os.makedirs(out_dir, exist_ok=True)
            
            # å¯é€‰: ä¿å­˜å•ç«™shapefile
            if SAVE_INDIVIDUAL_SHP:
                shp = os.path.join(out_dir, f"{code}_catchment.shp")
                cat.to_crs(4326).to_file(shp, driver="ESRI Shapefile", 
                                         encoding="utf-8")
            
            # ç»Ÿè®¡CSV
            stats_csv = os.path.join(out_dir, f"{code}_stats.csv")
            pd.DataFrame([{
                "code": code, "lon": lon, "lat": lat,
                "outlet_comid": outlet_comid, "snap_dist_m": dist_m,
                "outlet_order": ordv, "outlet_uparea_km2": upa,
                "n_upstream_reaches": len(visited),
                "area_calc_m2": area_m2, "area_table_m2": area_target_m2,
                "rel_error": rel_err
            }]).to_csv(stats_csv, index=False, encoding="utf-8-sig")
            
            # ç»˜å›¾
            try:
                gdf_pt = gpd.GeoDataFrame(
                    {"code":[code]}, 
                    geometry=[Point(lon, lat)], 
                    crs=4326
                )
                xmin, ymin, xmax, ymax = cat.total_bounds
                pad = max(xmax-xmin, ymax-ymin) * 0.15
                
                fig, ax = plt.subplots(figsize=(7.2, 7.2))
                china_prov.boundary.plot(ax=ax, linewidth=0.6, alpha=0.8)
                cat.boundary.plot(ax=ax, linewidth=1.8, color='red')
                gdf_pt.plot(ax=ax, markersize=30, color='blue', marker='o')
                
                ax.set_xlim(xmin-pad, xmax+pad)
                ax.set_ylim(ymin-pad, ymax+pad)
                ax.set_aspect("equal", adjustable="box")
                ax.grid(True, linewidth=0.3, alpha=0.3)
                ax.set_title(f"{code} â€” Upstream (COMID={outlet_comid})", 
                            fontsize=11)
                
                png = os.path.join(out_dir, f"{code}_map.png")
                plt.savefig(png, dpi=300, bbox_inches="tight")
                plt.close(fig)
            except Exception as e:
                png = f"[ç»˜å›¾å¤±è´¥] {e}"
                plt.close('all')  # ç¡®ä¿å…³é—­æ‰€æœ‰å›¾å½¢
        
        return {
            "code": code, "status": status_str,
            "lon": lon, "lat": lat,
            "area_calc_m2": area_m2, "area_table_m2": area_target_m2,
            "rel_error": rel_err, "shp": shp, "png": png, 
            "stats_csv": stats_csv,
            "gdf": cat.to_crs(4326) if pass_check else None  # ç”¨äºåç»­åˆå¹¶è¾“å‡º
        }
        
    except Exception as e:
        return {"code": code, "status": "fail", "msg": str(e)}

# ================= ä¸»æµç¨‹ =================
def main():
    with open(run_log_path, "w", encoding="utf-8") as f: 
        f.write("")
    
    log("="*60)
    log("MERIT-BasinsæµåŸŸæå–å·¥å…· - ä¼˜åŒ–ç‰ˆ")
    log("="*60)
    
    # [1/8] è¯»å–æµ‹ç«™ä¿¡æ¯
    log("[1/8] è¯»å–æµ‹ç«™ä¿¡æ¯ ...")
    sheet, df_info = read_site_info(excel_path)
    df_info["area_m2"] = normalize_area_to_m2(df_info["area"])
    
    # æ–­ç‚¹ç»­ä¼ : æ£€æŸ¥å·²å®Œæˆçš„ç«™ç‚¹
    summary_csv = os.path.join(out_root, "summary.csv")
    completed = set()
    if os.path.exists(summary_csv):
        try:
            df_prev = pd.read_csv(summary_csv)
            completed = set(df_prev[df_prev["status"]=="ok"]["code"].astype(str))
            log(f"    å‘ç°å·²å®Œæˆ {len(completed)} ä¸ªç«™ç‚¹ï¼Œå°†è·³è¿‡")
        except:
            pass
    
    df_info = df_info[~df_info["code"].isin(completed)]
    log(f"    å·¥ä½œè¡¨: {sheet}, å¾…å¤„ç†ç«™ç‚¹: {len(df_info)}")
    
    if df_info.empty:
        log("æ‰€æœ‰ç«™ç‚¹å·²å®Œæˆï¼Œé€€å‡º")
        return
    
    # [2/8] è¯»å–ç©ºé—´æ•°æ®
    log("[2/8] è¯»å–æ²³ç½‘/å•å…ƒæµåŸŸ/çœç•Œ ...")
    gdf_riv = ensure_wgs84(gpd.read_file(riv_shp))
    gdf_cat = ensure_wgs84(gpd.read_file(cat_shp))
    china_prov = ensure_wgs84(gpd.read_file(china_prov_shp))
    
    for req in ["COMID"]:
        if req not in gdf_riv.columns: 
            raise ValueError("æ²³ç½‘ç¼ºå°‘ COMID")
        if req not in gdf_cat.columns: 
            raise ValueError("å•å…ƒæµåŸŸç¼ºå°‘ COMID")
    
    log(f"    æ²³ç½‘: {len(gdf_riv)} æ¡, å•å…ƒæµåŸŸ: {len(gdf_cat)} ä¸ª")
    
    # [3/8] ğŸš€ é¢„è®¡ç®—æŠ•å½±æ•°æ® (å…³é”®ä¼˜åŒ–ç‚¹)
    log("[3/8] ğŸš€ é¢„è®¡ç®—æŠ•å½±æ•°æ® (å‡å°‘é‡å¤è½¬æ¢) ...")
    gdf_riv_m = gdf_riv.to_crs(3857)  # ç”¨äºè·ç¦»è®¡ç®—
    gdf_cat_area = gdf_cat.to_crs(AREA_EPSG)  # ç”¨äºé¢ç§¯è®¡ç®—
    log("    å®Œæˆ: æ²³ç½‘â†’3857, å•å…ƒæµåŸŸâ†’EPSG:6933")
    
    # [4/8] æ„å»ºæ‹“æ‰‘
    log("[4/8] æ„å»ºä¸Šæ¸¸æ‹“æ‰‘å›¾ ...")
    G = build_upstream_graph(gdf_riv)
    log(f"    æ‹“æ‰‘èŠ‚ç‚¹æ•°: {len(G)}")
    
    # [5/8] æ‰¹å¤„ç†
    log("[5/8] æ‰¹å¤„ç†æµ‹ç«™ ...")
    summary_rows = []
    all_catchments = []  # å­˜å‚¨æ‰€æœ‰æˆåŠŸçš„æµåŸŸ
    
    iterator = enumerate(df_info.itertuples(index=False), start=1)
    total = len(df_info)
    if HAS_TQDM:
        iterator = tqdm(iterator, total=total, desc="å¤„ç†è¿›åº¦", ncols=90)
    
    for idx, r in iterator:
        code = str(getattr(r, "code")).strip()
        lon  = float(getattr(r, "lon"))
        lat  = float(getattr(r, "lat"))
        area_tab = getattr(r, "area_m2")
        
        log(f"[{idx}/{total}] {code}")
        res = process_one_site(
            code, lon, lat, area_tab,
            gdf_riv_m, gdf_riv, gdf_cat, gdf_cat_area,
            china_prov, G
        )
        summary_rows.append(res)
        
        # æ”¶é›†æˆåŠŸçš„æµåŸŸç”¨äºåˆå¹¶è¾“å‡º
        if res.get("status") == "ok" and res.get("gdf") is not None:
            all_catchments.append(res["gdf"])
        
        # æ—¥å¿—
        if res.get("status") == "ok":
            log(f"  âœ“ OK | ç›¸å¯¹è¯¯å·®={fmt_pct(res.get('rel_error'))}")
        elif res.get("status") == "reject":
            log(f"  âœ— REJECT | ç›¸å¯¹è¯¯å·®={fmt_pct(res.get('rel_error'))}")
        elif res.get("status") == "fail":
            log(f"  âœ— FAIL | {res.get('msg')}")
        
        # å®šæœŸå†…å­˜æ£€æŸ¥
        if idx % MEMORY_CHECK_INTERVAL == 0:
            check_memory()
    
    # [6/8] è¾“å‡ºæ±‡æ€»
    log("[6/8] è¾“å‡ºæ±‡æ€»ç»“æœ ...")
    
    # æ±‡æ€»CSV
    df_summary = pd.DataFrame(summary_rows)
    df_summary.to_csv(summary_csv, index=False, encoding="utf-8-sig")
    log(f"    æ±‡æ€»è¡¨: {summary_csv}")
    
    # ğŸš€ æ‰€æœ‰æµåŸŸåˆå¹¶ä¸ºå•ä¸ªGeoPackage (å‡å°‘æ–‡ä»¶ç¢ç‰‡)
    if all_catchments:
        gpkg_path = os.path.join(out_root, "all_catchments.gpkg")
        gdf_all = gpd.GeoDataFrame(
            pd.concat(all_catchments, ignore_index=True),
            crs=4326
        )
        gdf_all.to_file(gpkg_path, driver="GPKG", layer="catchments")
        log(f"    âœ“ æ‰€æœ‰æµåŸŸGeoPackage: {gpkg_path} ({len(gdf_all)}ä¸ª)")

        # å†™å‡ºæ¯ä¸ªç«™ç‚¹çš„ç‹¬ç«‹ GeoPackageï¼ˆ.gpkgï¼‰
        log("    å†™å‡ºæ¯ç«™å•ç‹¬ GeoPackage ...")
        for i in range(len(gdf_all)):
            try:
                row_gdf = gdf_all.iloc[[i]].copy()
                sid = str(row_gdf.iloc[0].get("station_id", "")).strip()
                if not sid:
                    sid = f"site_{i+1}"
                out_dir_site = os.path.join(out_root, "sites", sid)
                os.makedirs(out_dir_site, exist_ok=True)
                gpkg_path = os.path.join(out_dir_site, f"{sid}_catchment.gpkg")
                # ä½¿ç”¨ GPKG å†™å‡ºï¼Œæ¯ä¸ªæ–‡ä»¶ä¸€ä¸ªå›¾å±‚
                row_gdf.to_file(gpkg_path, driver="GPKG", layer="catchment")
            except Exception as e:
                log(f"    å†™å‡ºç«™ç‚¹ {sid} GeoPackage å¤±è´¥: {e}")
    
    # [7/8] ç»Ÿè®¡å›¾
    log("[7/8] ç”Ÿæˆç»Ÿè®¡å›¾ ...")
    cnt = df_summary["status"].value_counts().reindex(
        ["ok","reject","fail"], fill_value=0
    )
    
    fig, ax = plt.subplots(figsize=(6,4))
    bars = ax.bar(cnt.index, cnt.values, color=['green','orange','red'])
    ax.set_ylabel("Count", fontsize=11)
    ax.set_title("æ‰¹å¤„ç†ç»“æœç»Ÿè®¡", fontsize=12)
    
    for bar, v in zip(bars, cnt.values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(v)}', ha='center', va='bottom')
    
    chart_path = os.path.join(out_root, "summary_chart.png")
    plt.savefig(chart_path, dpi=200, bbox_inches="tight")
    plt.close()
    log(f"    ç»Ÿè®¡å›¾: {chart_path}")
    
    # [8/8] å®Œæˆ
    log("="*60)
    log(f"[8/8] âœ… å®Œæˆ! è¾“å‡ºç›®å½•: {out_root}")
    log(f"    æˆåŠŸ: {cnt.get('ok',0)} | è¶…å·®: {cnt.get('reject',0)} | å¤±è´¥: {cnt.get('fail',0)}")
    log("="*60)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"âŒ ç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {e}")
        import traceback
        log(traceback.format_exc())
        raise
